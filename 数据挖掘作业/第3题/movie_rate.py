import numpy as np
import pandas as pd

# åˆ†ç¦»è®­ç»ƒé›†å’Œæµ‹è¯•é›†,
# ç»“è®º:è®­ç»ƒé›†80000*3,(æ¶‰åŠåˆ°8372éƒ¨ç”µå½±) æµ‹è¯•é›†20000*3,(æ¶‰åŠåˆ°4869éƒ¨ç”µå½±),
from sklearn.base import BaseEstimator, RegressorMixin, clone
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor
from sklearn.kernel_ridge import KernelRidge
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor, BayesianRidge
from sklearn.svm import SVR, LinearSVR
from surprise import KNNWithMeans, Reader, Dataset
from sklearn.model_selection import cross_val_score, GridSearchCV

# è¯»å–æ•°æ®,åˆ†ç¦»trainå’Œtesté›†
movie = pd.read_csv("/Users/zhouang/Desktop/æ•°æ®æŒ–æ˜ä½œä¸š/question3_recommend.csv")
test_index = (movie['rating'] == -1)
test_data = movie[test_index]
train_data = movie[~test_index]
train_data.to_csv('/Users/zhouang/Desktop/æ•°æ®æŒ–æ˜ä½œä¸š/ä½œä¸š3/train.csv', index=False)
test_data.to_csv('/Users/zhouang/Desktop/æ•°æ®æŒ–æ˜ä½œä¸š/ä½œä¸š3/test.csv', index=False)

# å‘ç°æµ‹è¯•é›†ä¸­çš„æŸäº›ç”µå½±åœ¨è®­ç»ƒé›†ä¸­æ ¹æœ¬ä¸å­˜åœ¨,æœ‰694éƒ¨. æ„å‘³ç€åªèƒ½é ççŒœ
m1 = set(train_data["movieId"].unique())
m2 = set(test_data["movieId"].unique())
gap = m2 - m1
print(len(gap))


#  è®¡ç®—å‡ºç”µå½±ç‰¹å¾è¡¨:å¹³å‡åˆ†,æœ€ä½,æœ€é«˜,ä¼—æ•°,è¯„è®ºäººæ•°
#  ç»“è®º:è®­ç»ƒé›†æ¶‰åŠåˆ°8372éƒ¨ç”µå½±,ä½†æ˜¯4040éƒ¨åªæœ‰â‰¤2äººæ¥è¯„åˆ†, ä¼°è®¡ç»Ÿè®¡åå·®ç›¸å½“ä¸¥é‡
def get_mode(s):
    return s.value_counts(ascending=False).index[0]


movie_grouped = train_data.groupby(by='movieId')
movie_feature = movie_grouped['rating'].agg(
    {'è¯„è®ºäººæ•°': 'count', "å¹³å‡åˆ†": 'mean', "ä¼—æ•°": get_mode, 'ä¸­ä½æ•°': 'median', 'æœ€é«˜åˆ†': 'max', 'æœ€ä½åˆ†': "min"})
movie_feature.to_csv('/Users/zhouang/Desktop/æ•°æ®æŒ–æ˜ä½œä¸š/ä½œä¸š3/ç”µå½±ç‰¹å¾.csv')  # å­˜ä¸€ä¸‹


# åˆ¶ä½œç”¨æˆ·ç‰¹å¾è¡¨:  ç”¨æˆ·ä¹è§‚åº¦(å¤§äºä¸­ä½æ•°çš„æ•°é‡å’Œé¢‘ç‡,å°äºä¸­ä½æ•°çš„æ•°é‡å’Œé¢‘ç‡), ç–‘ä¼¼æ°´å†›æŒ‡æ ‡,é»‘å­æŒ‡æ ‡
def positive_rate(DF):
    # è¯„åˆ†æ€»æ˜¯é«˜äºä¸­ä½æ•°çš„æ¦‚ç‡
    return pd.Series([np.sum((DF['rating'] - DF['ä¸­ä½æ•°']) >= 0) / DF.shape[0], DF.shape[0]], index=['å®¹å¿åº¦', 'è¯„è®ºä¸ªæ•°'])


# åˆ†ç»„ç»Ÿè®¡,æ„é€ ç‰¹å¾
user = pd.merge(train_data, movie_feature, left_on='movieId', right_index=True)
user_grouped = user.groupby(by='userId')
user_feature = user_grouped.apply(positive_rate)

high = 3.577777777777778
low = 2.3111111111111113


def high_score(DF):
    # å–œæ¬¢æ‰“é«˜åˆ†çš„æ¦‚ç‡
    row = np.sum((DF['rating'] >= high)) / DF.shape[0]
    return pd.Series(row, index=['é«˜åˆ†ç‡'])


user_feature['é«˜åˆ†ç‡'] = user_grouped.apply(high_score)


def get_black(DF):
    # å¯¹é«˜åˆ†ç”µå½±æ‰“ä½åˆ†çš„æ¦‚ç‡, 4åˆ†ä»¥ä¸Šä¸ºé«˜åˆ†ç”µå½±
    good_movie = (DF['å¹³å‡åˆ†'] >= high)
    good_size = np.sum(good_movie)
    return pd.Series([np.sum(DF[good_movie]['rating'] < low) / good_size, good_size], index=['é»‘è¯„ç‡', 'çœ‹è¿‡é«˜åˆ†ç”µå½±ä¸ªæ•°'])


user_feature = pd.concat([user_feature, user_grouped.apply(get_black)], axis=1)


def get_faker(DF):
    # å¯¹å¤§é‡ä½åˆ†ç”µå½±æ‰“é«˜åˆ†
    bad_movie = (DF['å¹³å‡åˆ†'] <= low)
    bad_size = np.sum(bad_movie)
    return pd.Series([np.sum(DF[bad_movie]['rating'] > high) / bad_size, bad_size], index=['æ°´å†›ç‡', 'çœ‹è¿‡ä½åˆ†ç”µå½±ä¸ªæ•°'])


user_feature = pd.concat([user_feature, user_grouped.apply(get_faker)], axis=1)

# æŠŠæ‰€æœ‰ç‰¹å¾éƒ½æ±‡æ€»åˆ°totalä¸­
total = pd.merge(user, user_feature, left_on='userId', right_index=True)


# é€‰å®šrmseä½œä¸ºæŒ‡æ ‡
def rmse_cv(model, X, y):
    # rmseä½œä¸ºè¯„åˆ†
    rmse = np.sqrt(-cross_val_score(model, X, y, scoring="neg_mean_squared_error", cv=5))
    return rmse


# å¢åŠ ååŒè¿‡æ»¤ä½œä¸ºç‰¹å¾
fuker = KNNWithMeans()
r = Reader(rating_scale=(0.5, 5))
data_set = Dataset.load_from_df(train_data, r)
knn_val = []
for index, row in train_data.iterrows():
    user_id, movie_id, rate = row['userId'], row['movieId'], row['rating']
    predict_rate = fuker.predict(user_id, movie_id, rate).est
    knn_val.append(predict_rate)
s = pd.Series(knn_val, index=train_data.index)
total['ååŒå€¼'] = s

# æœ‰äº›ç”¨æˆ·æ²¡çœ‹è¿‡å¥½ç”µå½± or  åƒåœ¾ç”µå½±. æ‰€ä»¥å°†NaNå¡«å……ä¸º0
total.fillna({'æ°´å†›ç‡': 0, 'é»‘è¯„ç‡': 0}, inplace=True)
X = total[['è¯„è®ºäººæ•°', 'å¹³å‡åˆ†', 'ä¼—æ•°', 'ä¸­ä½æ•°', 'æœ€é«˜åˆ†', 'æœ€ä½åˆ†', 'å®¹å¿åº¦', 'è¯„è®ºä¸ªæ•°', 'é«˜åˆ†ç‡', 'é»‘è¯„ç‡', 'çœ‹è¿‡é«˜åˆ†ç”µå½±ä¸ªæ•°', 'æ°´å†›ç‡', 'çœ‹è¿‡ä½åˆ†ç”µå½±ä¸ªæ•°', 'ååŒå€¼']]
Y = total['rating']

# ä¸åŠ ååŒåˆ—,ä½¿ç”¨LASSO,0.80-0.85ä¹‹é—´æµ®åŠ¨ , åªç”¨å¹³å‡æ•°å¤§çº¦0.85-0.9ä¹‹é—´, åŠ äº†ååŒåˆ—åœ¨0.63-0.67ä¹‹é—´æµ®åŠ¨
# ç”¨å…¶ä»–åˆ†ç±»å™¨è¯•è¯•
# æ€»ç»“:Extra,SGD,RF è®­ç»ƒç›¸å½“è€—æ—¶.æ‰€ä»¥ä¸é‡‡ç”¨.åªé‡‡ç”¨é‡‡ç”¨ç®€å•çš„çº¿æ€§å›å½’å¥½äº†
models = [LinearRegression(),
          Ridge(),
          Lasso(alpha=0.01, max_iter=1000),
          GradientBoostingRegressor(),
          ElasticNet(alpha=0.001, max_iter=1000),
          BayesianRidge()]

names = ["LR", "Ridge", "Lasso", "GBR", "Ela", "Bay"]
for name, model in zip(names, models):
    score = rmse_cv(model, X, Y)
    print("{}: {:.6f}, {:.4f}".format(name, score.mean(), score.std()))


# ç®€å•è¶…å‚æ•°æœç´¢
class grid:
    def __init__(self, model):
        self.model = model

    def grid_get(self, X, y, param_grid):
        grid_search = GridSearchCV(self.model, param_grid, cv=5, scoring="neg_mean_squared_error")
        grid_search.fit(X, y)
        print(grid_search.best_params_, np.sqrt(-grid_search.best_score_))
        grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])
        print(pd.DataFrame(grid_search.cv_results_)[['params', 'mean_test_score', 'std_test_score']])


# LASSO alpha=0.0001
grid(Lasso()).grid_get(X, Y, {'alpha': [0.0001, 0.0004, 0.0006, 0.0009], 'max_iter': [1000]})
# Ridge alpha=0.5
grid(Ridge()).grid_get(X, Y, {'alpha': [0.5, 1, 3, 4, 8, 10]})
# 'alpha': 0.0008, 'l1_ratio': 0.3
grid(ElasticNet()).grid_get(X, Y, {'alpha': [0.0008, 0.004, 0.005], 'l1_ratio': [0.08, 0.1, 0.3], 'max_iter': [1000]})


# æ¨¡å‹èåˆ
class AverageWeight(BaseEstimator, RegressorMixin):
    def __init__(self, mod, weight):
        self.mod = mod
        self.weight = weight

    def fit(self, X, y):
        self.models_ = [clone(x) for x in self.mod]
        for my_model in self.models_:
            my_model.fit(X, y)
        return self

    def predict(self, X):
        w = []
        pred = np.array([my_model.predict(X) for my_model in self.models_])
        for data in range(pred.shape[1]):
            single = [pred[model, data] * weight for model, weight in zip(range(pred.shape[0]), self.weight)]
            w.append(np.sum(single))
        return w


ols = LinearRegression()
lasso = Lasso(alpha=0.0001)
ridge = Ridge(alpha=0.5)
ela = ElasticNet(alpha=0.0008, l1_ratio=0.3, max_iter=1000)
gbr = GradientBoostingRegressor()
bay = BayesianRidge()

w_ols = 0.02
w_lasso = 0.03
w_ridge = 0.1
w_ela = 0.2
w_gbr = 0.45
w_bay = 0.2

weight_avg = AverageWeight(mod=[ols, lasso, ridge, ela, gbr, bay],
                           weight=[w_ols, w_lasso, w_ridge, w_ela, w_gbr, w_bay])
score = rmse_cv(weight_avg, X, Y)
# rmse=0.8148411905749648,å‡‘åˆç”¨å§   ä¼˜åŒ–é«˜åˆ†ä¸º0.8127, å¢åŠ ååŒåˆ—åä¸º0.608
print(score.mean())

# å¯¹testæ•°æ®è¿›è¡Œmerge,æ„æˆå®Œæ•´çš„ç‰¹å¾
total_test = pd.merge(test_data, movie_feature, left_on='movieId', right_index=True, how='left')
total_test = pd.merge(total_test, user_feature, left_on='userId', right_index=True, how='left')

# å¢åŠ ååŒåˆ—
knn_val = []
for index, row in total_test.iterrows():
    user_id = row['userId']
    movie_id = row['movieId']
    rate = row['rating']
    predict = fuker.predict(user_id, movie_id, rate)
    if predict.details['was_impossible']:
        knn_val.append(None)
    else:
        knn_val.append(predict.est)
s1 = pd.Series(knn_val, index=total_test.index)
total_test['ååŒå€¼'] = s1

# å¡«å……ç¼ºå¤±å€¼,ç”±äºæœ‰äº›ç”µå½±ç¼ºå¤±,æ‰€ä»¥ç›´æ¥é‡‡ç”¨ 3ä½œä¸ºå¹³å‡å€¼,è°ƒæ•´äº†ä¸€ä¸‹æœ€ä½åˆ†
na_value = 3
fill_value = {'è¯„è®ºäººæ•°': 1, "å¹³å‡åˆ†": na_value, 'ä¸­ä½æ•°': na_value, 'ä¼—æ•°': na_value, 'æœ€é«˜åˆ†': na_value, 'æœ€ä½åˆ†': 1.5, 'é»‘è¯„ç‡': 0.0,
              'æ°´å†›ç‡': 0.0, 'ååŒå€¼': na_value}
total_test.fillna(fill_value, inplace=True)

# è®­ç»ƒ,é¢„æµ‹,å­˜å‚¨
select_col = ['è¯„è®ºäººæ•°', 'å¹³å‡åˆ†', 'ä¼—æ•°', 'ä¸­ä½æ•°', 'æœ€é«˜åˆ†', 'æœ€ä½åˆ†', 'å®¹å¿åº¦', 'è¯„è®ºä¸ªæ•°', 'é«˜åˆ†ç‡', 'é»‘è¯„ç‡', 'çœ‹è¿‡é«˜åˆ†ç”µå½±ä¸ªæ•°', 'æ°´å†›ç‡', 'çœ‹è¿‡ä½åˆ†ç”µå½±ä¸ªæ•°',
              'ååŒå€¼']
weight_avg.fit(X, Y)
result = weight_avg.predict(total_test[select_col])
total_test['æœ€åç»“æœ'] = result

fuker = total_test[['userId', 'movieId', 'æœ€åç»“æœ']]


# æ›¿æ¢ <0.5 å’Œ å¤§äº5çš„å€¼
def sub(x):
    if x >= 5:
        return 5.0
    elif x <= 0.5:
        return 0.5
    return x


fuker['æœ€åç»“æœ'] = fuker['æœ€åç»“æœ'].map(sub)
fuker.rename(columns={'æœ€åç»“æœ': 'rating'}, inplace=True)
# ä¿å­˜
fuker.to_csv("/Users/zhouang/Desktop/æ•°æ®æŒ–æ˜ä½œä¸š/ä½œä¸š3/é¢„æµ‹ç»“æœ.csv", index=False)

# åæœŸæ”¹è¿›,
# 1.å“ªäº›é«˜åˆ†ç”µå½±,å“ªäº›æ˜¯ä½åˆ†ç”µå½±,å¯ä»¥æä¸€ä¸ªæœç´¢æ¥ç¡®å®šé¢„æµ‹æœ€ä¼˜å€¼çš„è®¾å®šè¿™2ä¸ªå€¼ (ç»“è®º:é«˜åˆ†æœç´¢[3.5-4.2],ä½åˆ†æœç´¢[2,2.7ä¹‹é—´],æœ€ä¼˜ç»“æœ high:3.578 low:2.311)
# 2.æµ‹è¯•é›†ä¸­çš„ç‹¬æœ‰ç”µå½±,å‡åˆ†éšä¾¿è®¤å®šäº†ä¸€ä¸ª3åˆ†, è¿™ä¸ªæš‚æ—¶æ²¡æƒ³å¥½æ€ä¹ˆä¼˜åŒ–
# 3.å¯ä»¥å¼•å…¥å…³è”,çœ‹å“ªäº›ç”¨æˆ·å£å‘³ç›¸ä¼¼. åŠ å…¥ ç›¸ä¼¼ç”¨æˆ·æ‰€æ‰“çš„å¹³å‡åˆ†ä½œä¸ºä¸€ä¸ªfeature. (ç»“è®º:å¢åŠ ååŒåˆ—,æœ€åäº¤å‰éªŒè¯ä¸º0.61,å¯æƒœæäº¤æ—¶é—´è¿‡äº†,æ²¡æœ‰æµ‹è¯•,å“ğŸ˜”)
